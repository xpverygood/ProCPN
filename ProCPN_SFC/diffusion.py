import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom helpers import (    cosine_beta_schedule,    linear_beta_schedule,    vp_beta_schedule,    extract,    Losses)from utils import Progress, Silentclass Diffusion(nn.Module):    def __init__(self, state_dim, action_dim, model, max_action=1.0, # max_action is now less relevant for logits                 beta_schedule='vp', n_timesteps=5,                 loss_type='l2', clip_denoised=True, # Consider if clip_denoised is still needed for logits                 bc_coef=0.0): # Keep bc_coef=0 for noise prediction        super(Diffusion, self).__init__()        self.state_dim = state_dim        self.action_dim = action_dim        self.model = model # This should be the modified MLP from model.py        if beta_schedule == 'linear':            betas = linear_beta_schedule(n_timesteps)        elif beta_schedule == 'cosine':            betas = cosine_beta_schedule(n_timesteps)        elif beta_schedule == 'vp':            betas = vp_beta_schedule(n_timesteps)        else:             raise ValueError(f"Unknown beta schedule: {beta_schedule}")        alphas = 1. - betas        alphas_cumprod = torch.cumprod(alphas, axis=0)        alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])        self.n_timesteps = int(n_timesteps)        self.clip_denoised = False # Explicitly disable internal clamping for logits        self.bc_coef = bc_coef        self.register_buffer('betas', betas)        self.register_buffer('alphas_cumprod', alphas_cumprod)        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)        posterior_variance = torch.clamp(posterior_variance, min=1e-20)        self.register_buffer('posterior_variance', posterior_variance)        self.register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance)) # Already clamped        self.register_buffer('posterior_mean_coef1',                             betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))        self.register_buffer('posterior_mean_coef2',                             (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))        self.loss_fn = Losses[loss_type]()    def predict_start_from_noise(self, x_t, t, noise):        return (                extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -                extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise        )    def q_posterior(self, x_start, x_t, t):        """ Calculate parameters of the posterior distribution q(x_{t-1} | x_t, x_0) """        posterior_mean = (                extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +                extract(self.posterior_mean_coef2, t, x_t.shape) * x_t        )        posterior_variance = extract(self.posterior_variance, t, x_t.shape)        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)        return posterior_mean, posterior_variance, posterior_log_variance_clipped    def p_mean_variance(self, x, t, s):        """ Calculate the mean and variance of the reverse process step p(x_{t-1} | x_t) """        pred = self.model(x, t, s) # s is the conditioning state        if self.bc_coef == 0.0: # Model predicts noise            x_recon = self.predict_start_from_noise(x, t=t, noise=pred)        else: # Model predicts x0 directly            x_recon = pred        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)        return model_mean, posterior_variance, posterior_log_variance    # @torch.no_grad() # Keep gradients for policy training    def p_sample(self, x, t, s):        """ Sample x_{t-1} from the reverse process distribution p(x_{t-1} | x_t) """        b, *_, device = *x.shape, x.device        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, s=s)        noise = torch.randn_like(x)        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise    # @torch.no_grad() # Keep gradients for policy training    def p_sample_loop(self, state, shape, verbose=False, return_diffusion=False):        """ Iteratively sample from p(x_{t-1} | x_t) to generate x0 from noise """        device = self.betas.device        batch_size = shape[0]        x = torch.randn(shape, device=device) # Start from pure noise N(0, I)        if torch.isnan(x).any() or torch.isinf(x).any():            print("!!! ERROR: NaN/Inf in initial noise 'x' !!!")        if return_diffusion: diffusion = [x]        progress = Progress(self.n_timesteps) if verbose else Silent()        for i in reversed(range(0, self.n_timesteps)):            timesteps = torch.full((batch_size,), i, device=device, dtype=torch.long)            x = self.p_sample(x, timesteps, state) # state is the conditioning input 's'            progress.update({'t': i})            if return_diffusion: diffusion.append(x)        progress.close()        if return_diffusion:            return x, torch.stack(diffusion, dim=1)        else:            return x    # @torch.no_grad() # Keep gradients for policy training    def sample(self, state, *args, **kwargs):        """ Generate a sample (e.g., action logits) conditioned on the state """        batch_size = state.shape[0]        shape = (batch_size, self.action_dim)        action_params = self.p_sample_loop(state, shape, *args, **kwargs)        return action_params # Return the raw output (logits)    def q_sample(self, x_start, t, noise=None):        """ Forward process: Add noise to x_start to get x_t """        if noise is None:            noise = torch.randn_like(x_start)        sample = (                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +                extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise        )        return sample    def p_losses(self, x_start, state, t, weights=1.0):        """ Calculate the diffusion loss for training """        noise = torch.randn_like(x_start)        # Get noisy version of x_start        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)        # Predict noise (or x0) using the internal model        x_recon_or_noise = self.model(x_noisy, t, state)        assert noise.shape == x_start.shape        if self.bc_coef == 0.0: # Model predicts noise, loss is on noise            loss = self.loss_fn(x_recon_or_noise, noise, weights)        else: # Model predicts x0, loss is on x0             loss = self.loss_fn(x_recon_or_noise, x_start, weights)        return loss    def loss(self, x, state, weights=1.0):        """ Calculate the overall diffusion loss for a batch """        batch_size = len(x)        # Sample random timesteps for each item in the batch        t = torch.randint(0, self.n_timesteps, (batch_size,), device=x.device).long()        return self.p_losses(x, state, t, weights)    def forward(self, state, *args, **kwargs):        """ Alias for sample method, used by policy's forward pass """        return self.sample(state, *args, **kwargs)